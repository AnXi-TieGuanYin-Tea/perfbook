\QuickQAC{chp:Introduction}
\QuickQ{Come on now!!!
	Parallel programming has been known to be exceedingly
	hard for many decades.
	So just why are you trying to tell me differently now?}

	If you really believe that parallel programming is exceedingly
	hard, then you should have a ready answer to the question
	``Why is parallel programming hard?''
	One could list any number of reasons, ranging from deadlocks to
	race conditions to testing coverage, but the real answer is that
	{\em it is not really all that hard}.
	After all, if parallel programming was really so horribly difficult,
	how could a large number of open-source projects, ranging from Apache
	to MySQL to the Linux kernel, have managed to master it?

	A better question might be: ''Why is parallel programming {\em
	perceived} to be so difficult?''
	To see the answer, let's go back to the year 1991.
	Paul McKenney was walking across the parking lot to Sequent's
	benchmarking center carrying six dual-80486 Sequent Symmetry CPU
	boards, when he suddenly realized that he was carrying several
	times the price of the house he had just purchased.\footnote{
		Yes, this sudden realization {\em did} cause him to walk quite
		a bit more carefully.
		Why do you ask?}
	This high cost of parallel systems meant that
	parallel programming was restricted to a privileged few who
	worked for an employer who either manufactured or could afford to
	purchase machines costing upwards of \$100,000 --- in 1991 dollars US.

	In contrast, in 2006, Paul finds himself typing these words on a
	dual-core x86 laptop.
	Unlike the dual-80486 CPU boards, this laptop also contains
	2GB of main memory, a 60GB disk drive, a display, Ethernet,
	USB ports, wireless, and Bluetooth.
	And the laptop is more than an order of magnitude cheaper than
	even one of those dual-80486 CPU boards, even before taking inflation
	into account.

	Parallel systems have truly arrived.
	They are no longer the sole domain of a privileged few, but something
	available to almost everyone.

	The earlier restricted availability of parallel hardware is
	the \emph{real} reason that parallel programming is considered
	so difficult.
	After all, it is quite difficult to learn to program even the simplest
	machine if you have no access to it.
	Since the age of rare and expensive parallel machines is for the most
	part behind us, the age during which
	parallel programming is perceived to be mind-crushingly difficult is
	coming to a close.\footnote{
		Parallel programming is in some ways more difficult than
		sequential programming, for example, parallel validation
		is more difficult.
		But no longer mind-crushingly difficult.}

\QuickQ{How could parallel programming \emph{ever} be as easy
	   as sequential programming???}

	   It depends on the programming environment.
	   SQL~\cite{DIS9075SQL92} is an underappreciated success
	   story, as it permits programmers who know nothing about parallelism
	   to keep a large parallel system productively busy.
	   We can expect more variations on this theme as parallel
	   computers continue to become cheaper and more readily available.
	   For example, one possible contender in the scientific and
	   technical computing arena is Matlab*p @@@ cite @@@,
	   which is an attempt to automatically parallelize comon
	   matrix operations.

\QuickQ{Where are the answers to the Quick Quizzes found?}

	In Appendix~\ref{chp:Answers to Quick Quizzes} starting on
	page~\pageref{chp:Answers to Quick Quizzes}.

	Hey, I thought I owed you an easy one!!!

\QuickQAC{chp:Hardware and its Habits}
\QuickQ{Why should parallel programmers bother learning low-level
	properties of the hardware?
	Wouldn't it be easier, better, and more general to remain at
	a higher level of abstraction?}

	It might well be easier to ignore the detailed properties of
	the hardware, but in most cases it would be quite foolish
	to do so.
	If you accept that the only purpose of parallelism is to
	increase performance, and if you further accept that
	performance depends on detailed properties of the hardware,
	then it logically follows that parallel programmers are going
	to need to know at least a few hardware properties.

	This is the case in most engineering disciplines.
	Would \emph{you} want to use a bridge designed by an
	engineer who did not understand the properties of
	the concrete and steel making up that bridge?
	If not, why would you expect a parallel programmer to be
	able to develop competent parallel software without some
	understanding of the underlying hardware?

\QuickQ{What types of machines would allow atomic operations on
	multiple data elements?}

	One answer to this question is that it is often possible to
	pack multiple elements of data into a single machine word,
	which can then be manipulated atomically.

	A more trendy answer would be machines supporting transactional
	memory~\cite{DBLomet1977SIGSOFT}.
	However, such machines are still (as of 2008) research
	curiosities.
	The jury is still out on the applicability of transactional
	memory~\cite{McKenney2007PLOSTM,DonaldEPorter2007TRANSACT,
	ChistopherJRossbach2007a}.

\QuickQAC{cha:SMP Synchronization Design}
\QuickQ{In what situation would hierarchical locking work well?}

	If the comparison on line~31 of
	Figure~\ref{fig:SMPdesign:Hierarchical-Locking Hash Table Search}
	were replaced by a much heavier-weight operation,
	then releasing {\tt bp->bucket\_lock} \emph{might} reduce lock
	contention enough to outweigh the overhead of the extra
	acquisition and release of {\tt cur->node\_lock}.

\QuickQ{In Figure~\ref{fig:SMPdesign:Allocator Cache Performance},
	there is a pattern of performance rising with increasing run
	length in groups of three samples, for example, for run lengths
	10, 11, and 12.
	Why?}

	This is due to the per-CPU target value being three.
	A run length of 12 must acquire the global-pool lock twice,
	while a run length of 13 must acquire the global-pool lock
	three times.

\QuickQ{Allocation failures were observed in the two-thread
	tests at run lengths of 19 and greater.
	Given the global-pool size of 40 and the per-CPU target
	pool size of three, what is the smallest allocation run
	length at which failures can occur?}

	The exact solution to this problem is left as an exercise to
	the reader.
	The first solution received will be credited to its submitter.
	As a rough rule of thumb, the global pool size should be at least
	$m+2sn$, where
	``m'' is the maximum number of elements allocated at a given time,
	``s'' is the per-CPU pool size,
	and ``n'' is the number of CPUs.

\QuickQAC{chp:defer:Deferred Processing}
\QuickQ{Why not implement reference-acquisition using
	a simple compare-and-swap operation that only
	acquires a reference if the reference counter is
	non-zero?}

	Although this can resolve the race between the release of
	the last reference and acquisition of a new reference,
	it does absolutely nothing to prevent the data structure
	from being freed and reallocated, possibly as some completely
	different type of structure.
	It is quite likely that the ``simple compare-and-swap
	operation'' would give undefined results if applied to the
	differently typed structure.

	In short, use of atomic operations such as compare-and-swap
	absolutely requires either type-safety or existence guarantees.

\QuickQ{Why isn't it necessary to guard against cases where
	   one CPU acquires a reference just after another
	   CPU releases the last reference?}

	  Because a CPU must already hold a reference in order
	  to legally acquire another reference.
	  Therefore, if one CPU releases the last reference,
	  there cannot possibly be any CPU that is permitted
	  to acquire a new reference.
	  This same fact allows the non-atomic check in line~22
	  of Figure~\ref{fig:defer:Linux Kernel kref API}.

\QuickQ{If the check on line~22 of
	   Figure~\ref{fig:defer:Linux Kernel kref API} fails, how
	   could the check on line~23 possibly succeed?}

	  Suppose that {\tt kref\_put()} is protected by RCU, so
	  that two CPUs might be executing line~22 concurrently.
	  Both might see the value ``2'', causing both to then
	  execute line~23.
	  One of the two instances of {\tt atomic\_dec\_and\_test()}
	  will decrement the value to zero and thus return 1.

\QuickQ{How can it possibly be safe to non-atomically check
	   for equality with ``1'' on line~22 of
	   Figure~\ref{fig:defer:Linux Kernel kref API}?}

	  Remember that it is not legal to call either {\tt kref\_get()}
	  or {\tt kref\_put()} unless you hold a reference.
	  If the reference count is equal to ``1'', then there
	  can't possibly be another CPU authorized to change the
	  value of the reference count.

\QuickQ{Why can't the check for a zero reference count be
	   made in a simple ``if'' statement with an atomic
	   increment in its ``then'' clause?}

	  Suppose that the ``if'' condition completed, finding
	  the reference counter value equal to one.
	  Suppose that a release operation executes, decrementing
	  the reference counter to zero and therefore starting
	  cleanup operations.
	  But now the ``then'' clause can increment the counter
	  back to a value of one, allowing the object to be
	  used after it has been cleaned up.

\QuickQ{But doesn't seqlock also permit readers and updaters to get
work done concurrently?}

Yes and no.
Although seqlock readers can run concurrently with
seqlock writers, whenever this happens, the {\tt read\_seqretry()}
primitive will force the reader to retry.
This means that any work done by a seqlock reader running concurrently
with a seqlock updater will be discarded and redone.
So seqlock readers can \emph{run} concurrently with updaters,
but they cannot actually get any work done in this case.

In contrast, RCU readers can perform useful work even in presence
of concurrent RCU updaters.

\QuickQ{What prevents the {\tt list\_for\_each\_entry\_rcu()} from
getting a segfault if it happens to execute at exactly the same
time as the {\tt list\_add\_rcu()}?}

On all systems running Linux, loads from and stores
to pointers are atomic, that is, if a store to a pointer occurs at
the same time as a load from that same pointer, the load will return
either the initial value or the value stored, never some bitwise mashup
of the two.
In addition, the {\tt list\_for\_each\_entry\_rcu()} always proceeds
forward through the list, never looking back.
Therefore, the {\tt list\_for\_each\_entry\_rcu()} will either see
the element being added by {\tt list\_add\_rcu()} or it will not,
but either way, it will see a valid well-formed list.

\QuickQ{Why do we need to pass two pointers into
{\tt hlist\_for\_each\_entry\_rcu()}
when only one is needed for {\tt list\_for\_each\_entry\_rcu()}?}

Because in an hlist it is necessary to check for
NULL rather than for encountering the head.
(Try coding up a single-pointer {\tt hlist\_for\_each\_entry\_rcu()}
If you come up with a nice solution, it would be a very good thing!)

\QuickQ{How would you modify the deletion example to permit more than two
versions of the list to be active?}

One way of accomplishing this is as shown in
Figure~\ref{fig:defer:Concurrent RCU Deletion}.

\begin{figure}[htbp]
{ \centering
\begin{verbatim}
  1 spin_lock(&mylock);
  2 p = search(head, key);
  3 if (p == NULL)
  4   spin_unlock(&mylock);
  5 else {
  6   list_del_rcu(&p->list);
  7   spin_unlock(&mylock);
  8   synchronize_rcu();
  9   kfree(p);
 10 }
\end{verbatim}
}
\caption{Concurrent RCU Deletion}
\label{fig:defer:Concurrent RCU Deletion}
\end{figure}

Note that this means that multiple concurrent deletions might be
waiting in {\tt synchronize\_rcu()}.

\QuickQ{How many RCU versions of a given list can be
active at any given time?}

That depends on the synchronization design.
If a semaphore protecting the update is held across the grace period,
then there can be at most two versions, the old and the new.

However, if only the search, the update, and the
{\tt list\_replace\_rcu()} were protected by a lock, then
there could be an arbitrary number of versions active, limited only
by memory and by how many updates could be completed within a
grace period.
But please note that data structures that are updated so frequently
probably are not good candidates for RCU.
That said, RCU can handle high update rates when necessary.

\QuickQ{How can RCU updaters possibly delay RCU readers, given that the
{\tt rcu\_read\_lock()} and {\tt rcu\_read\_unlock()}
primitives neither spin nor block?}

The modifications undertaken by a given RCU updater will cause the
corresponding CPU to invalidate cache lines containing the data,
forcing the CPUs running concurrent RCU readers to incur expensive
cache misses.
(Can you design an algorithm that changes a data structure \emph{without}
inflicting expensive cache misses on concurrent readers?
On subsequent readers?)

\QuickQ{
WTF???
How the heck do you expect me to believe that RCU has a
100-femtosecond overhead when the clock period at 3GHz is more than
300 \emph{picoseconds}?}

First, consider that the inner loop used to
take this measurement is as follows:

\vspace{5pt}
\begin{minipage}[t]{\columnwidth}
\small
\begin{verbatim}
  1 for (i = 0; i < CSCOUNT_SCALE; i++) {
  2   rcu_read_lock();
  3   rcu_read_unlock();
  4 }
\end{verbatim}
\end{minipage}
\vspace{5pt}

Next, consider the effective definitions of \url{rcu_read_lock()}
and \url{rcu_read_unlock()}:

\vspace{5pt}
\begin{minipage}[t]{\columnwidth}
\small
\begin{verbatim}
  1 #define rcu_read_lock()   do { } while (0)
  2 #define rcu_read_unlock() do { } while (0)
\end{verbatim}
\end{minipage}
\vspace{5pt}

Consider also that the compiler does simple optimizations,
allowing it to replace the loop with:

\vspace{5pt}
\begin{minipage}[t]{\columnwidth}
\small
\begin{verbatim}
i = CSCOUNT_SCALE;
\end{verbatim}
\end{minipage}
\vspace{5pt}

So the "measurement" of 100 femtoseconds is simply the fixed
overhead of the timing measurements divided by the number of
passes through the inner loop containing the calls
to \url{rcu_read_lock()} and \url{rcu_read_unlock()}.
And therefore, this measurement really is in error, in fact,
in error by an arbitrary number of orders of magnitude.
As you can see by the definition of \url{rcu_read_lock()}
and \url{rcu_read_unlock()} above, the actual overhead
is precisely zero.

It certainly is not every day that a timing measurement of
100 femtoseconds turns out to be an overestimate!

\QuickQ{
Why does both the variability and overhead of rwlock decrease as the
critical-section overhead increases?}

Because the contention on the underlying
\url{rwlock_t} decreases as the critical-section overhead
increases.
However, the rwlock overhead will not quite drop to that on a single
CPU because of cache-thrashing overhead.

\QuickQ{
Is there an exception to this deadlock immunity, and if so,
what sequence of events could lead to deadlock?}

One way to cause a deadlock cycle involving
RCU read-side primitives is via the following (illegal) sequence
of statements:

\vspace{5pt}
\begin{minipage}[t]{\columnwidth}
\small
\begin{verbatim}
idx = srcu_read_lock(&srcucb);
synchronize_srcu(&srcucb);
srcu_read_unlock(&srcucb, idx);
\end{verbatim}
\end{minipage}
\vspace{5pt}

The \url{synchronize_rcu()} cannot return until all
pre-existing SRCU read-side critical sections complete, but
is enclosed in an SRCU read-side critical section that cannot
complete until the \url{synchronize_srcu()} returns.
The result is a classic self-deadlock--you get the same
effect when attempting to write-acquire a reader-writer lock
while read-holding it.

Note that this self-deadlock scenario does not apply to
RCU Classic, because the context switch performed by the
\url{synchronize_rcu()} would act as a quiescent state
for this CPU, allowing a grace period to complete.
However, this is if anything even worse, because data used
by the RCU read-side critical section might be freed as a
result of the grace period completing.

In short, do not invoke synchronous RCU update-side primitives
from within an RCU read-side critical section.

\QuickQ{
But wait!
This is exactly the same code that might be used when thinking
of RCU as a replacement for reader-writer locking!
What gives?}

This is an effect of the Law of Toy Examples:
beyond a certain point, the code fragments look the same.
The only difference is in how we think about the code.
However, this difference can be extremely important.
For but one example of the importance, consider that if we think
of RCU as a restricted reference counting scheme, we would never
be fooled into thinking that the updates would exclude the RCU
read-side critical sections.
\\ ~ \\
It nevertheless is often useful to think of RCU as a replacement
for reader-writer locking, for example, when you are replacing reader-writer
locking with RCU.

\QuickQ{
Why the dip in refcnt overhead near 6 CPUs?}

Most likely NUMA effects.
However, there is substantial variance in the values measured for the
refcnt line, as can be seen by the error bars.
In fact, standard deviations range in excess of 10% of measured
values in some cases.
The dip in overhead therefore might well be a statistical aberration.

\QuickQ{
	But what if there is an arbitrarily long series of RCU
	read-side critical sections in multiple threads, so that at
	any point in time there is at least one thread in the system
	executing in an RCU read-side critical section?
	Wouldn't that prevent any data from a \url{SLAB_DESTROY_BY_RCU}
	slab ever being returned to the system, possibly resulting
	in OOM events?}

	There could certainly be an arbitrarily long period of time
	during which at least one thread is always in an RCU read-side
	critical section.
	However, the key words in the description in
	Section~\ref{sec:deferRCU is a Way of Providing Type-Safe Memory}
	are ``in-use'' and ``pre-existing''.
	Keep in mind that a given RCU read-side critical section is
	conceptually only permitted to gain references to data elements
	that were in use at the beginning of that critical section.
	Furthermore, remember that a slab cannot be returned to the
	system until all of its data elements have been freed, in fact,
	the RCU grace period cannot start until after they have all been
	freed.

	Therefore, the slab cache need only wait for those RCU read-side
	critical sections that started before the freeing of the last element
	of the slab.
	This in turn means that any RCU grace period that begins after
	the freeing of the last element will do---the slab may be returned
	to the system after that grace period ends.

\QuickQ{
Suppose that the \url{nmi_profile()} function was preemptible.
What would need to change to make this example work correctly?}

One approach would be to use
\url{rcu_read_lock()} and \url{rcu_read_unlock()}
in \url{nmi_profile()}, and to replace the
\url{synchronize_sched()} with \url{synchronize_rcu()},
perhaps as shown in
Figure~\ref{fig:defer:Using RCU to Wait for Mythical Preemptable NMIs to Finish}.
\\ ~ \\
\begin{figure}[tbp]
{ \tt \scriptsize
\begin{verbatim}
  1 struct profile_buffer {
  2   long size;
  3   atomic_t entry[0];
  4 };
  5 static struct profile_buffer *buf = NULL;
  6 
  7 void nmi_profile(unsigned long pcvalue)
  8 {
  9   struct profile_buffer *p;
 10 
 11   rcu_read_lock();
 12   p = rcu_dereference(buf);
 13   if (p == NULL) {
 14     rcu_read_unlock();
 15     return;
 16   }
 17   if (pcvalue >= p->size) {
 18     rcu_read_unlock();
 19     return;
 20   }
 21   atomic_inc(&p->entry[pcvalue]);
 22   rcu_read_unlock();
 23 }
 24 
 25 void nmi_stop(void)
 26 {
 27   struct profile_buffer *p = buf;
 28 
 29   if (p == NULL)
 30     return;
 31   rcu_assign_pointer(buf, NULL);
 32   synchronize_rcu();
 33   kfree(p);
 34 }
\end{verbatim}
}
\caption{Using RCU to Wait for Mythical Preemptable NMIs to Finish}
\label{fig:defer:Using RCU to Wait for Mythical Preemptable NMIs to Finish}
\end{figure}


\QuickQ{Why do some of the cells in
	Table~\ref{tab:defer:RCU Wait-to-Finish APIs}
	have exclamation marks (``!'')?}

	The API members with exclamation marks (\url{rcu_read_lock()},
	\url{rcu_read_unlock()}, and \url{call_rcu()}) were the
	only members of the Linux RCU API that Paul E. McKenney was aware
	of back in the mid-90s.
	During this timeframe, he was under the mistaken impression that
	he knew all that there is to know about RCU.

\QuickQ{The \url{synchronize_rcu()} API waits for all pre-existing
	interrupt handlers to complete, right?}

	Absolutely not!!!
	And especially not when using preemptable RCU!
	You instead want \url{synchronize_irq()}.
	Alternatively, you can place calls to \url{rcu_read_lock()}
	and \url{rcu_read_unlock()} in the specific interrupt handlers that
	you want \url{synchronize_rcu()} to wait for.

\QuickQ{What happens if you mix and match?
	For example, suppose you use \url{rcu_read_lock()} and
	\url{rcu_read_unlock()} to delimit RCU read-side critical
	sections, but then use \url{call_rcu_bh()} to post an
	RCU callback?}

	If there happened to be no RCU read-side critical
	sections delimited by \url{rcu_read_lock_bh()} and
	\url{rcu_read_unlock_bh()} at the time \url{call_rcu_bh()}
	was invoked, RCU would be within its rights to invoke the callback
	immediately, possibly freeing a data structure still being used by
	the RCU read-side critical section!
	This is not merely a theoretical possibility: a long-running RCU
	read-side critical section delimited by \url{rcu_read_lock()}
	and \url{rcu_read_unlock()} is vulnerable to this failure mode.

	This vulnerability disappears in -rt kernels, where
	RCU Classic and RCU BH both map onto a common implementation.

\QuickQ{Hardware interrupt handlers can be thought of as being
	under the protection of an implicit \url{rcu_read_lock_bh()},
	right?}

	Absolutely not!!!
	And especially not when using preemptable RCU!
	If you need to access ``rcu\_bh''-protected data structures
	in an interrupt handler, you need to provide explicit calls to
	\url{rcu_read_lock_bh()} and \url{rcu_read_unlock_bh()}.

\QuickQ{What happens if you mix and match RCU Classic and RCU Sched?}

	In a non-\url{PREEMPT} or a \url{PREEMPT} kernel, mixing these
	two works "by accident" because in those kernel builds, RCU Classic
	and RCU Sched map to the same implementation.
	However, this mixture is fatal in \url{PREEMPT_RT} builds using the -rt
	patchset, due to the fact that Realtime RCU's read-side critical
	sections can be preempted, which would permit
	\url{synchronize_sched()} to return before the
	RCU read-side critical section reached its \url{rcu_read_unlock()}
	call.
	This could in turn result in a data structure being freed before the
	read-side critical section was finished with it,
	which could in turn greatly increase the actuarial risk experienced
	by your kernel.

	In fact, the split between RCU Classic and RCU Sched was inspired
	by the need for preemptible RCU read-side critical sections.

\QuickQ{In general, you cannot rely on \url{synchronize_sched()} to
	wait for all pre-existing interrupt handlers,
	right?}

	That is correct!
	Because -rt Linux uses threaded interrupt handlers, there can
	be context switches in the middle of an interrupt handler.
	Because \url{synchronize_sched()} waits only until each
	CPU has passed through a context switch, it can return
	before a given interrupt handler completes.

	If you need to wait for a given interrupt handler to complete,
	you should instead use \url{synchronize_irq()} or place
	explicit RCU read-side critical sections in the interrupt
	handlers that you wish to wait on.

\QuickQ{Why do both SRCU and QRCU lack asynchronous \url{call_srcu()}
or \url{call_qrcu()} interfaces?}

Given an asynchronous interface, a single task
could register an arbitrarily large number of SRCU or QRCU callbacks,
thereby consuming an arbitrarily large quantity of memory.
In contrast, given the current synchronous
\url{synchronize_srcu()} and \url{synchronize_qrcu()}
interfaces, a given task must finish waiting for a given grace period
before it can start waiting for the next one.

\QuickQ{Under what conditions can \url{synchronize_srcu()} be safely
used within an SRCU read-side critical section?}

In principle, you can use
\url{synchronize_srcu()} with a given \url{srcu_struct}
within an SRCU read-side critical section that uses some other
\url{srcu_struct}.
In practice, however, doing this is almost certainly a bad idea.
In particular, the code shown in
Figure~\ref{fig:defer:Multistage SRCU Deadlocks}
could still result in deadlock.

\begin{figure}[htbp]
{ \centering
\begin{verbatim}
  1 idx = srcu_read_lock(&ssa);
  2 synchronize_srcu(&ssb);
  3 srcu_read_unlock(&ssa, idx);
  4 
  5 /* . . . */
  6 
  7 idx = srcu_read_lock(&ssb);
  8 synchronize_srcu(&ssa);
  9 srcu_read_unlock(&ssb, idx);
\end{verbatim}
}
\caption{Multistage SRCU Deadlocks}
\label{fig:defer:Multistage SRCU Deadlocks}
\end{figure}


\QuickQ{Why doesn't \url{list_del_rcu()} poison both the \url{next}
and \url{prev} pointers?}

Poisoning the \url{next} pointer would interfere
with concurrent RCU readers, who must use this pointer.
However, RCU readers are forbidden from using the \url{prev}
pointer, so it may safely be poisoned.

\QuickQ{Normally, any pointer subject to \url{rcu_dereference()} \emph{must}
always be updated using \url{rcu_assign_pointer()}.
What is an exception to this rule?}

One such exception is when a multi-element linked
data structure is initialized as a unit while inaccessible to other
CPUs, and then a single \url{rcu_assign_pointer()} is used
to plant a global pointer to this data structure.
The initialization-time pointer assignments need not use
\url{rcu_assign_pointer()}, though any such assignments that
happen after the structure is globally visible \url{must} use
\url{rcu_assign_pointer()}.
\\
However, unless this initialization code is on an impressively hot
code-path, it is probably wise to use \url{rcu_assign_pointer()}
anyway, even though it is in theory unnecessary.
It is all too easy for a "minor" change to invalidate your cherished
assumptions about the initialization happening privately.

\QuickQ{Are there any downsides to the fact that these traversal and update
primitives can be used with any of the RCU API family members?}

It can sometimes be difficult for automated
code checkers such as ``sparse'' (or indeed for human beings) to
work out which type of RCU read-side critical section a given
RCU traversal primitive corresponds to.
For example, consider the code shown in
Figure~\ref{fig:defer:Diverse RCU Read-Side Nesting}.

\begin{figure}[htbp]
{ \centering
\begin{verbatim}
  1 rcu_read_lock();
  2 preempt_disable();
  3 p = rcu_dereference(global_pointer);
  4 
  5 /* . . . */
  6 
  7 preempt_enable();
  8 rcu_read_unlock();
\end{verbatim}
}
\caption{Diverse RCU Read-Side Nesting}
\label{fig:defer:Diverse RCU Read-Side Nesting}
\end{figure}

Is the \url{rcu_dereference()} primitive in an RCU Classic
or an RCU Sched critical section?
What would you have to do to figure this out?

\QuickQ{Why wouldn't any deadlock in the RCU implementation in
	Figure~\ref{fig:defer:Lock-Based RCU Implementation}
	also be a deadlock in any other RCU implementation?}


\begin{figure}[tbp]
{ \scriptsize
\begin{verbatim}
  1 void foo(void)
  2 {
  3   spin_lock(&my_lock);
  4   rcu_read_lock();
  5   do_something();
  6   rcu_read_unlock();
  7   do_something_else();
  8   spin_unlock(&my_lock);
  9 }
 10 
 11 void bar(void)
 12 {
 13   rcu_read_lock();
 14   spin_lock(&my_lock);
 15   do_some_other_thing();
 16   spin_unlock(&my_lock);
 17   do_whatever();
 18   rcu_read_unlock();
 19 }
\end{verbatim}
}
\caption{Deadlock in Lock-Based RCU Implementation}
\label{fig:defer:Deadlock in Lock-Based RCU Implementation}
\end{figure}

	Suppose the functions \url{foo()} and \url{bar()} in
	Figure~\ref{fig:defer:Deadlock in Lock-Based RCU Implementation}
	are invoked concurrently from different CPUs.
	Then \url{foo()} will acquire \url{my_lock()} on line~3,
	while \url{bar()} will acquire \url{rcu_gp_lock} on
	line~13.
	When \url{foo()} advances to line~4, it will attempt to
	acquire \url{rcu_gp_lock}, which is held by \url{bar()}.
	Then when \url{bar()} advances to line~14, it will attempt
	to acquire \url{my_lock}, which is held by \url{foo()}.

	Each function is then waiting for a lock that the other
	holds, a classic deadlock.

	Other RCU implementations neither spin nor block in
	\url{rcu_read_lock()}, hence avoiding deadlocks.

\QuickQ{Why not simply use reader-writer locks in the RCU implementation
	in
	Figure~\ref{fig:defer:Lock-Based RCU Implementation}
	in order to allow RCU readers to proceed in parallel?}

	One could in fact use reader-writer locks in this manner.
	However, textbook reader-writer locks suffer from memory
	contention, so that the RCU read-side critical sections would
	need to be quite long to actually permit parallel execution.
	@@@ add reference to reader-writer locking discussion from LJ2003 @@@

	On the other hand, use of a reader-writer lock that is
	read-acquired in \url{rcu_read_lock()} would avoid the
	deadlock condition noted above.

\QuickQ{How can the grace period possibly elapse in 40 nanoseconds when
	\url{synchronize_rcu()} contains a 10-millisecond delay?}

	The update-side test was run in absence of readers, so the
	\url{poll()} system call was never invoked.
	In addition, the actual code has this \url{poll()}
	system call commented out, the better to evaluate the
	true overhead of the update-side code.
	Any production uses of this code would be better served by
	using the \url{poll()} system call, but then again,
	production uses would be even better served by other implementations
	shown later in this section.

\QuickQ{Why not simply make \url{rcu_read_lock()} wait when a concurrent
	\url{synchronize_rcu()} has been waiting too long in
	the RCU implementation in
	Figure~\ref{fig:defer:RCU Implementation Using Single Global Reference Counter}?
	Wouldn't that prevent \url{synchronize_rcu()} from starving?}

	Although this would in fact eliminate the starvation, it would
	also mean that \url{rcu_read_lock()} would spin or block waiting
	for the writer, which is in turn waiting on readers.
	If one of these readers is attempting to acquire a lock that
	the spinning/blocking \url{rcu_read_lock()} holds, we again
	have deadlock.

	In short, the cure is worse than the disease.
	See Section~\ref{defer:Starvation-Free Counter-Based RCU}
	for a proper cure.

\QuickQ{Why the memory barrier on line~5 of \url{synchronize_rcu()} in
	Figure~\ref{fig:defer:RCU Update Using Global Reference-Count Pair}
	given that there is a spin-lock acquisition immediately after?}

	The spin-lock acquisition only guarantees that the spin-lock's
	critical section will not ``bleed out'' to precede the
	acquisition.
	It in no way guarantees that code preceding the spin-lock
	acquisitoin won't be reordered into the critical section.
	Such reordering could cause a removal from an RCU-protected
	list to be reordered to follow the complementing of
	\url{rcu_idx}, which could allow a newly starting RCU
	read-side critical section to see the recently removed
	data element.

	Exercise for the reader: use a tool such as Promela/spin
	to determine which (if any) of the memory barriers in
	Figure~\ref{fig:defer:RCU Update Using Global Reference-Count Pair}
	are really needed.
	See Section~\ref{app:formal:Formal Verification}
	for information on using these tools.
	The first correct and complete response will be credited.

\QuickQ{Why is the counter flipped twice in
	Figure~\ref{fig:defer:RCU Update Using Global Reference-Count Pair}?
	Shouldn't a single flip-and-wait cycle be sufficient?}

	Both flips are absolutely required.
	To see this, consider the following sequence of events:
	\begin{enumerate}
	\item	Line~8 of \url{rcu_read_lock()} in
		Figure~\ref{fig:defer:RCU Read-Side Using Global Reference-Count Pair}
		picks up \url{rcu_idx}, finding its value to be zero.
	\item	Line~8 of \url{synchronize_rcu()} in
		Figure~\ref{fig:defer:RCU Update Using Global Reference-Count Pair}
		complements the value of \url{rcu_idx}, setting its
		value to one.
	\item	Lines~10-13 of \url{synchronize_rcu()} find that the
		value of \url{rcu_refcnt[0]} is zero, and thus
		returns.
		(Recall that the question is asking what happens if
		lines~14-20 are omitted.)
	\item	Lines~9 and 10 of \url{rcu_read_lock()} store the
		value zero to this thread's instance of \url{rcu_read_idx}
		and increments \url{rcu_refcnt[0]}, respectively.
		Execution then proceeds into the RCU read-side critical
		section.
		\label{defer:rcu_rcgp:RCU Read Side Start}
	\item	Another instance of \url{synchronize_rcu()} again complements
		\url{rcu_idx}, this time setting its value to zero.
		Because \url{rcu_refcnt[1]} is zero, \url{synchronize_rcu()}
		returns immediately.
		(Recall that \url{rcu_read_lock()} incremented
		\url{rcu_refcnt[0]}, not \url{rcu_refcnt[1]}!)
		\label{defer:rcu_rcgp:RCU Grace Period Start}
	\item	The grace period that started in
		step~\ref{defer:rcu_rcgp:RCU Grace Period Start}
		has been allowed to end, despite
		the fact that the RCU read-side critical section
		that started beforehand in
		step~\ref{defer:rcu_rcgp:RCU Read Side Start}
		has not completed.
		This violates RCU semantics, and could allow the update
		to free a data element that the RCU read-side critical
		section was still referencing.
	\end{enumerate}

	Exercise for the reader: What happens if \url{rcu_read_lock()}
	is preempted for a very long time (hours!) just after
	line~8?
	Does this implementation operate correctly in that case?
	Why or why not?
	The first correct and complete response will be credited.

\QuickQ{Given that atomic increment and decrement are so expensive,
	why not just use non-atomic increment on line~10 and a
	non-atomic decrement on line~25 of
	Figure~\ref{fig:defer:RCU Read-Side Using Global Reference-Count Pair}?}

	Using non-atomic operations would cause increments and decrements
	to be lost, in turn causing the implementation to fail.
	See Section~\ref{defer:Scalable Counter-Based RCU}
	for a safe way to use non-atomic operations in
	\url{rcu_read_lock()} and \url{rcu_read_unlock()}.

\QuickQ{Come off it!
	We can see the \url{atomic_read()} primitive in
	\url{rcu_read_lock()}!!!
	So why are you trying to pretend that \url{rcu_read_lock()}
	contains no atomic operations???}

	The \url{atomic_read()} primitives does not actually execute
	atomic machine instructions, but rather does a normal load
	from an \url{atomic_t}.

\QuickQ{Great, if we have $N$ threads, we can have $2N$ ten-millisecond
	waits (one set per \url{flip_counter_and_wait()} invocation,
	and even that assumes that we wait only once for each thread.
	Don't we need the grace period to complete \emph{much} more quickly?}

	Keep in mind that we only wait for a given thread if that thread
	is still in a pre-existing RCU read-side critical section,
	and that waiting for one hold-out thread gives all the other
	threads a chance to complete any pre-existing RCU read-side
	critical sections that they might still be executing.
	So the only way that we would wait for $2N$ intervals
	would be if the last thread still remained in a pre-existing
	RCU read-side critical section despite all the waiting for
	all the prior threads.
	In short, this implementation will not wait unnecessarily.

	However, if you are stress-testing code that uses RCU, you
	might want to comment out the \url{poll()} statement in
	order to better catch bugs that incorrectly retain a reference
	to an RCU-protected data element outside of an RCU
	read-side critical section.

\QuickQ{All of these toy RCU implementations have either atomic operations
	in \url{rcu_read_lock()} and \url{rcu_read_unlock()},
	or \url{synchronize_rcu()}
	overhead that increases linearly with the number of threads.
	Under what circumstances could an RCU implementation enjoy
	light-weight implementations for all three of these primitives,
	all having deterministic ($O(1)$) overheads and latencies?}

	Special-purpose uniprocessor implementations of RCU can attain
	this ideal.
	See @@@.

\QuickQ{If any even value is sufficient to tell \url{synchronize_rcu()}
	to ignore a given task, why doesn't line~10 of
	Figure~\ref{fig:defer:Free-Running Counter Using RCU}
	simply assign zero to \url{rcu_reader_gp}?}

	Assigning zero (or any other even-numbered constant)
	would in fact work, but assigning the value of
	\url{rcu_gp_ctr} can provide a valuable debugging aid,
	as it gives the developer an idea of when the corresponding
	thread last exited an RCU read-side critical section.

\QuickQ{Why are the memory barriers on lines~17 and 29 of
	Figure~\ref{fig:defer:Free-Running Counter Using RCU}
	needed?
	Aren't the memory barriers inherent in the locking
	primitives on lines~18 and 28 sufficient?}

	These memory barriers are required because the locking
	primitives are only guaranteed to confine the critical
	section.
	The locking primitives are under absolutely no obligation
	to keep other code from bleeding in to the critical section.
	The pair of memory barriers are therefore requires to prevent
	this sort of code motion, whether performed by the compiler
	or by the CPU.

\QuickQ{Couldn't the update-side optimization described in
	Section~\ref{defer:Scalable Counter-Based RCU With Shared Grace Periods}
	be applied to the implementation shown in
	Figure~\ref{fig:defer:Free-Running Counter Using RCU}?}

	Indeed it could, with a few modifications.
	This work is left as an exercise for the reader.

\QuickQ{Why not simply maintain a separate per-thread nesting-level
	variable, as was done in previous section, rather than having
	all this complicated bit manipulation?}

	The apparent simplicity of the separate per-thread variable
	is a red herring.
	This approach incurs much greater complexity in the guise
	of careful ordering of operations, especially if signal
	handlers are to be permitted to contain RCU read-side
	critical sections.
	But don't take my word for it, code it up and see what you
	end up with!

\QuickQ{Given the algorithm shown in
	Figure~\ref{fig:defer:Nestable RCU Using a Free-Running Counter},
	how could you double the time required to overflow the global
	\url{rcu_gp_ctr}?}

	One way would be to replace the magnitude comparison on
	lines~33 and 34 with an inequality check of the per-thread
	\url{rcu_reader_gp} variable against
	\url{rcu_gp_ctr+RCU_GP_CTR_BOTTOM_BIT}.

\QuickQ{Again, given the algorithm shown in
	Figure~\ref{fig:defer:Nestable RCU Using a Free-Running Counter},
	is counter overflow fatal?
	Why or why not?
	If it is fatal, what can be done to fix it?}

	It can indeed be fatal.
	To see this, consider the following sequence of events:
	\begin{enumerate}
	\item	Thread~0 enters \url{rcu_read_lock()}, determines
		that it is not nested, and therefore fetches the
		value of the global \url{rcu_gp_ctr}.
		Thread~0 is then preempted for an extremely long time
		(before storing to its per-thread \url{rcu_reader_gp}
		variable).
	\item	Other threads repeatedly invoke \url{synchronize_rcu()},
		so that the new value of the global \url{rcu_gp_ctr}
		is now \url{RCU_GP_CTR_BOTTOM_BIT}
		less than it was when thread~0 fetched it.
	\item	Thread~0 now starts running again, and stores into
		its per-thread \url{rcu_reader_gp} variable.
		The value it stores is
		\url{RCU_GP_CTR_BOTTOM_BIT+1}
		greater than that of the global \url{rcu_gp_ctr}.
	\item	Thread~0 acquires a reference to RCU-protected data
		element~A.
	\item	Thread 1 now removes the data element~A that thread~0
		just acquired a reference to.
	\item	Thread 1 invokes \url{synchronize_rcu()}, which
		increments the global \url{rcu_gp_ctr} by
		\url{RCU_GP_CTR_BOTTOM_BIT}.
		It then checks all of the per-thread \url{rcu_reader_gp}
		variables, but thread~0's value (incorrectly) indicates
		that it started after thread~1's call to
		\url{syncrhonize_rcu()}, so thread~1 does not wait
		for thread~0 to complete its RCU read-side critical
		section.
	\item	Thread 1 then frees up data element~A, which thread~0
		is still referencing.
	\end{enumerate}

	Note that scenario can also occur in the implementation presented in
	Section~\ref{defer:RCU Based on Free-Running Counter}.

	One strategy for fixing this problem is to use 64-bit
	counters so that the time required to overflow them would exceed
	the useful lifetime of the computer system.
	Note that non-antique members of the 32-bit x86 CPU family 
	allow atomic manipulation of 64-bit counters via the
	\url{cmpxchg64b} instruction.

	Another strategy is to limit the rate at which grace periods are
	permitted to occur in order to achieve a similar effect.
	For example, \url{synchronize_rcu()} could record the last time
	that it was invoked, and any subsequent invocation would then
	check this time and block as needed to force the desired
	spacing.
	For example, if the low-order four bits of the counter were
	reserved for nesting, and if grace periods were permitted to
	occur at most ten times per second, then it would take more
	than 300 days for the counter to overflow.
	However, this approach is not helpful if there is any possibility
	that the system will be fully loaded with CPU-bound high-priority
	real-time threads for the full 300 days.
	(A remote possibility, perhaps, but best to consider it ahead
	of time.)

	A third approach is to adminstratively abolish real-time threads
	from the system in question.
	In this case, the preempted process will age up in priority,
	thus getting to run long before the counter had a chance to
	overflow.
	Of course, this approach is less than helpful for real-time
	applications.

	A final approach would be for \url{rcu_read_lock()} to recheck
	the value of the global \url{rcu_gp_ctr} after storing to its
	per-thread \url{rcu_reader_gp} counter, retrying if the new
	value of the global \url{rcu_gp_ctr} is inappropriate.
	This works, but introduces non-deterministic execution time
	into \url{rcu_read_lock()}.
	On the other hand, if your application is being preempted long
	enough for the counter to overflow, you have no hope of
	deterministic execution time in any case!

\QuickQ{Doesn't the additional memory barrier shown on line~19 of
	Figure~\ref{fig:defer:RCU Read Side Using Quiescent States},
	greatly increase the overhead of \url{rcu_quiescent_state}?}

	Indeed it does!
	An application using this implementation of RCU should therefore
	invoke \url{rcu_quiescent_state} sparingly, instead using
	\url{rcu_read_lock()} and \url{rcu_read_unlock()} most of the
	time.

	Alternatively, the application might ensure that all counters
	are at least 64 bits wide in order to prevent overflow from
	happening during the machine's lifetime.
	However, given 64-bit counters, a simpler algorithm is possible
	(see for example \url{rcu64.h} and \url{rcu64.c}).

\QuickQ{Why are the two memory barriers on lines~30 and 31 of
	Figure~\ref{fig:defer:RCU Read Side Using Quiescent States}
	needed?}

	The memory barrier on line~30 prevents any RCU read-side
	critical sections that might precede the
	call to \url{rcu_thread_offline()} won't be reordered by either
	the compiler or the CPU to follow the assignment on line~31.
	The memory barrier on line~31 is, strictly speaking, unnecessary,
	as it is illegal to have any RCU read-side critical sections
	following the call to \url{rcu_thread_offline()}.

\QuickQ{To be sure, the clock frequencies of ca-2008 Power
	systems were quite high, but even a 5GHz clock
	frequency is insufficent to allow
	loops to be executed in 50~picoseconds!
	What is going on here?}

	Since the measurement loop contains a pair of empty functions,
	the compiler optimizes it away.
	The measurement loop takes 1,000 passes between each call to
	\url{rcu_quiescent_state()}, so this measurement is roughly
	one thousandth of the overhead of a single call to
	\url{rcu_quiescent_state()}.

\QuickQ{Why would the fact that the code is in a library make
	any difference for how easy it is to use the RCU
	implementation shown in
	Figures~\ref{fig:defer:RCU Read Side Using Quiescent States} and
	\ref{fig:defer:RCU Update Side Using Quiescent States}?}

	A library function has absolutely no control over the caller,
	and thus cannot force the caller to invoke \url{rcu_quiescent_state()}
	periodically.
	On the other hand, a library function that made many references
	to a given RCU-protected data structure might be able to invoke
	\url{rcu_thread_online()} upon entry,
	\url{rcu_quiescent_state()} periodically, and
	\url{rcu_thread_offline()} upon exit.

\QuickQAC{sec:advsync:Advanced Synchronization}
\QuickQ{How on earth could the assertion on line~21 of the code in
	Figure~\ref{fig:advsync:Parallel Hardware is Non-Causal} on
	page~\pageref{fig:advsync:Parallel Hardware is Non-Causal}
	\emph{possibly} fail???}

	The key point is that the intuitive analysis missed is that
	there is nothing preventing the assignment to C from overtaking
	the assignment to A as both race to reach {\tt thread2()}.
	This is explained in the remainder of this section.

\QuickQ{Great...  So how do I fix it?}

	The easiest fix is to replace the \url{barrier()} on
	line~12 with an \url{smp_mb()}.

\QuickQ{What assumption is the code fragment
	   in Figure~\ref{fig:advsync:Software Logic Analyzer}
	   making that might not be valid on real hardware?}

	   The code assumes that as soon as a given CPU stops
	   seeing its own value, it will immediately see the
	   final agreed-upon value.
	   On real hardware, some of the CPUs might well see several
	   intermediate results before converging on the final value.

\QuickQ{How could CPUs possibly have different views of the
	   value of a single variable \emph{at the same time?}}

	   Many CPUs have write buffers that record the values of
	   recent writes, which are applied once the corresponding
	   cache line makes its way to the CPU.
	   Therefore, it is quite possible for each CPU to see a
	   different value for a given variable at a single point
	   in time --- and for main memory to hold yet another value.
	   One of the reasons that memory barriers were invented was
	   to allow software to deal gracefully with situations like
	   this one.

\QuickQ{Why do CPUs~2 and 3 come to agreement so quickly, when it
	   takes so long for CPUs~1 and 4 to come to the party?}

	   CPUs~2 and 3 are a pair of hardware threads on the same
	   core, sharing the same cache hierarchy, and therefore have
	   very low communications latencies.
	   This is a NUMA, or, more accurately, a NUCA effect.

	   This leads to the question of why CPUs~2 and 3 ever disagree
	   at all.
	   One possible reason is that they each might have a small amount
	   of private cache in addition to a larger shared cache.
	   Another possible reason is instruction reordering, given the
	   short 10-nanosecond duration of the disagreement and the
	   total lack of memory barriers in the code fragment.

\QuickQ{But if the memory barriers do not unconditionally force
	ordering, how the heck can a device driver reliably execute
	sequences of loads and stores to MMIO registers???}

	MMIO registers are special cases: because they appear
	in uncached regions of physical memory.
	Memory barriers \emph{do} unconditionally force ordering
	of loads and stores to uncached memory.
	See Section~@@@ for more information on memory barriers
	and MMIO regions.

\QuickQ{How could the assertion {\tt b==2} on
	page~\pageref{codesample:advsync:What Can You Count On? 1}
	possibly fail?}

	If the CPU is not required to see all of its loads and
	stores in order, then the {\tt b=1+a} might well see an
	old version of the variable ``a''.
	
	This is why it is so very important that each CPU or thread
	see all of its own loads and stores in program order.

\QuickQ{How could the code on 
	page~\pageref{codesample:advsync:What Can You Count On? 2}
	possibly leak memory?}

	Only the first execution of the critical section should
	see {\tt p==NULL}.
	However, if there is no global ordering of critical sections for
	{\tt mylock}, then how can you say that a particular one was
	first?
	If several different executions of that critical section thought
	that they were first, they would all see {\tt p==NULL}, and
	they would all allocate memory.
	All but one of those allocations would be leaked.
	
	This is why it is so very important that all the critical sections
	for a given exclusive lock appear to execute in some well-defined
	order.

\QuickQ{How could the code on 
	page~\pageref{codesample:advsync:What Can You Count On? 2}
	possibly count backwards?}

	Suppose that the counter started out with the value zero,
	and that three executions of the critical section had therefore
	brought its value to three.
	If the fourth execution of the critical section is not constrained
	to see the most recent store to this variable, it might well see
	the original value of zero, and therefore set the counter to
	one, which would be going backwards.
	
	This is why it is so very important that loads from a given variable
	in a given critical
	section see the last store from the last prior critical section to
	store to that variable.

\QuickQ{What effect does the following sequence have on the
	order of stores to variables ``a'' and ``b''? \\
	{\tt ~~~~a = 1;} \\
	{\tt ~~~~b = 1;} \\
	{\tt ~~~~<write barrier>}}

	Absolutely none.  This barrier {\em would} ensure that the
	assignments to ``a'' and ``b'' happened before any subsequent
	assignments, but it does nothing to enforce any order of
	assignments to ``a'' and ``b'' themselves.

\QuickQ{What sequence of LOCK-UNLOCK operations \emph{would}
	act as a full memory barrier?}

	A series of two back-to-back LOCK-UNLOCK operations, or, somewhat
	less conventionally, an UNLOCK operations followed by a LOCK
	operation.

\QuickQ{What (if any) CPUs have memory-barrier instructions
	from which these semi-permiable locking primitives might
	be constructed?}

	Itanium is one example.
	The identification of any others is left as an
	exercise for the reader.

\QuickQ{Given that operations grouped in curly braces are executed
	concurrently, which of the rows of
	Table~\ref{tab:advsync:Lock-Based Critical Sections}
	are legitimate reorderings of the assignments to variables
	``A'' through ``F'' and the LOCK/UNLOCK operations?
	(The order in the code is A, B, LOCK, C, D, UNLOCK, E, F.)
	Why or why not?}

	\begin{enumerate}
	\item	Legitimate, executed in order.
	\item	Legitimate, the lock acquisition was executed concurrently
		with the last assignment preceding the critical section.
	\item	Illegitimate, the assignment to ``F'' must follow the LOCK
		operation.
	\item	Illegitimate, the LOCK must complete before any operation in
		the critical section.  However, the UNLOCK may legitimately
		be executed concurrently with subsequent operations.
	\item	Legitimate, the assignment to ``A'' precedes the UNLOCK,
		as required, and all other operations are in order.
	\item	Illegitimate, the assignment to ``C'' must follow the LOCK.
	\item	Illegitimate, the assignment to ``D'' must precede the UNLOCK.
	\item	Legitimate, all assignments are ordered with respect to the
		LOCK and UNLOCK operations.
	\item	Illegitimate, the assignment to ``A'' must precede the UNLOCK.
	\end{enumerate}

\QuickQ{What are the constraints for 
	Table~\ref{tab:advsync:Lock-Based Critical Sections}?}

	They are as follows:
	\begin{enumerate}
	\item	LOCK M must precede B, C, and D.
	\item	UNLOCK M must follow A, B, and C.
	\item	LOCK Q must precede F, G, and H.
	\item	UNLOCK Q must follow E, F, and G.
	\end{enumerate}

\QuickQAC{chp:Ease of Use}
\QuickQ{Can a similar algorithm be used when deleting elements?}

	Yes.
	However, since each thread must hold the locks of three
	consecutive elements to delete the middle one, if there
	are $N$ threads, there must be $2N+1$ elements (rather than
	just $N+1$ in order to avoid deadlock.

\QuickQ{Yetch!!!
	What ever possessed someone to come up with an algorithm
	that deserves to be shaved as much as this one does???}

	That would be Paul.

	He was considering the \emph{Dining Philosopher's Problem}, which
	involves a rather unsanitary spaghetti dinner attended by
	five philosphers.
	Given that there are five plates and but five forks on the table, and
	given that each philosopher requires two forks at a time to eat,
	one is supposed to come up with a fork-allocation algorithm that
	avoids deadlock.
	Paul's response was ``Sheesh!!!  Just get five more forks!!!''.

	This in itself was OK, but Paul then applied this same solution to
	circular linked lists.

	This would not have been so bad either, but he had to go and tell
	someone about it!!!

\QuickQ{Give an exception to this rule.}

	One exception would be a difficult and complex algorithm that
	was the only one known to work in a given situation.
	Another exception would be a difficult and complex algorithm
	that was nonetheless the simplest of the set known to work in
	a given situation.
	However, even in these cases, it may be very worthwhile to spend
	a little time trying to come up with a simpler algorithm!
	After all, if you managed to invent the first algorithm
	to do some task, it shouldn't be that hard to go on to
	invent a simpler one.

\QuickQAC{cha:app:Important Questions}
\QuickQ{What SMP coding errors can you see in these examples?
See @@@ for full code.}

(1)	Missing barrier() or volatile on tight loops.
(2)	Missing Memory barriers on update side.
(3)	Lack of synchronization between producer and consumer.

\QuickQ{How could there be such a large gap between successive
consumer reads?
See @@@ for full code.}

(1)	The consumer might be preempted for long time periods.
(2)	A long-running interrupt might delay the consumer.
(3)	The producer might also be running on a faster CPU than is the
	consumer (for example, one of the CPUs might have had to decrease its
	clock frequency due to heat-dissipation or power-consumption
	constraints).

\QuickQAC{app:primitives:Synchronization Primitives}
\QuickQ{Give an example of a parallel program that could be written
	   without synchronization primitives.}

	   There are many examples.
	   One of the simplest would be a parametric study using a
	   single independent variable.
	   If the program {\tt run\_study} took a single argument,
	   then we could use the following bash script to run two
	   instances in parallel, as might be appropriate on a
	   two-CPU system:

	   { \scriptsize \tt run\_study 1 > 1.out\& run\_study 2 > 2.out; wait}

	   One could of course argue that the bash ampersand operator and
	   the ``wait'' primitive are in fact synchronization primitives.
	   If so, then consider that 
	   this script could be run manually in two separate
	   command windows, so that the only synchronization would be
	   supplied by the user himself or herself.

\QuickQ{What problems could occur if the variable {\tt counter} were
	incremented without the protection of {\tt mutex}?}

	On CPUs with load-store architectures, incrementing {\tt counter}
	might compile into something like the following:

\vspace{5pt}
\begin{minipage}[t]{\columnwidth}
\small 
\begin{verbatim}
LOAD counter,r0
INC r0
STORE r0,counter
\end{verbatim}
\end{minipage} 
\vspace{5pt}

	On such machines, two threads might simultaneously load the
	value of {\tt counter}, each increment it, and each store the
	result.
	The new value of {\tt counter} will then only be one greater
	than before, despite two threads each incrementing it.

\QuickQ{How could you work around the lack of a per-thread-variable
	API on systems that do not provide it?}

	One approach would be to create an array indexed by
	{\tt smp\_thread\_id()}, and another would be to use a hash
	table to map from {\tt smp\_thread\_id()} to an array
	index --- which is in fact what this
	set of APIs does in pthread environments.

	Another approach would be for the parent to allocate a structure
	containing fields for each desired per-thread variable, then
	pass this to the child during thread creation.
	However, this approach can impose large software-engineering
	costs in large systems.
	To see this, imagine if all global variables in a large system
	had to be declared in a single file, regardless of whether or
	not they were C static variables!

\QuickQAC{chp:app:whymb:Why Memory Barriers?}
\QuickQ{What happens if two CPUs attempt to invalidate the
same cache line concurrently?}
One of the CPUs gains access
to the shared bus first,
and that CPU ``wins''.  The other CPU must invalidate its copy of the
cache line and transmit an ``invalidate acknowledge'' message
to the other CPU. \\
Of course, the losing CPU can be expected to immediately issue a
``read invalidate'' transaction, so the winning CPU's victory will
be quite ephemeral.

\QuickQ{When an ``invalidate'' message appears in a
	   large multiprocessor, every CPU must give an ``invalidate
	   acknowledge'' response.  Wouldn't the resulting ``storm''
	   of ``invalidate acknowledge'' responses totally saturate the
	   system bus?}

	It might, if large-scale multiprocessors were in fact implemented
	that way.  Larger multiprocessors, particularly NUMA machines,
	tend to use so-called ``directory-based'' cache-coherence
	protocols to avoid this and other problems.

\QuickQ{If SMP machines are really using message passing
	   anyway, why bother with SMP at all?}

	There has been quite a bit of controversy on this topic over
	the past few decades.  One answer is that the cache-coherence
	protocols are quite simple, and therefore can be implemented
	directly in hardware, gaining bandwidths and latencies
	unattainable by software message passing.  Another answer is that
	the real truth is to be found in economics due to the relative
	prices of large SMP machines and that of clusters of smaller
	SMP machines.  A third answer is that the SMP programming
	model is easier to use than that of distributed systems, but
	a rebuttal might note the appearance of HPC clusters and MPI.
	And so the argument continues.

\QuickQ{How does the hardware handle the delayed transitions
	   described above?}

	Usually by adding additional states, though these additional
	states need not be actually stored with the cache line, due to
	the fact that only a few lines at a time will be transitioning.
	The need to delay transitions is but one issue that results in
	real-world cache coherence protocols being much more complex than
	the over-simplified MESI protocol described in this appendix.
	Hennessy and Patterson's classic introduction to computer
	architecture~\cite{Hennessy95a} covers many of these issues.

\QuickQ{What sequence of operations would put the CPUs' caches
	   all back into the ``invalid'' state?}

	There is no such sequence, at least in absence of special
	``flush my cache'' instructions in the CPU's instruction set.
	Most CPUs do have such instructions.

\QuickQ{Does the guarantee that each CPU sees its own memory accesses
	   in order also guarantee that each user-level thread will see
	   its own memory accesses in order?  Why or why not?}

	No.  Consider the case where a thread migrates from one CPU to
	another, and where the destination CPU perceives the source
	CPU's recent memory operations out of order.  To preserve
	user-mode sanity, kernel hackers must use memory barriers in
	the context-switch path.  However, the locking already required
	to safely do a context switch should automatically provide
	the memory barriers needed to cause the user-level task to see
	its own accesses in order.  That said, if you are designing a
	super-optimized scheduler, either in the kernel or at user level,
	please keep this scenario in mind!

\QuickQ{Could this code be fixed by inserting a memory barrier
between CPU~1's ``while'' and assignment to ``c''?  Why or why not?}

No.  Such a memory barrier would only force ordering local to CPU~1.
It would have no effect on the relative ordering of CPU~0's and
CPU~1's accesses, so the assertion could still fire.

\QuickQ{Suppose that lines~3-5 for CPUs~1 and 2 are in an interrupt
handler, and that the CPU~2's line~9 is run at process level.
What changes, if any, are required to enable the code to work
correctly, in other words, to prevent the assertion from firing?}

The assertion will need to coded so as to ensure that the load of
``e'' precedes that of ``a''.
In the Linux kernel, the barrier() primitive may be used to accomplish
this in much the same way that the memory barrier was used in the
assertions in the previous examples.

\QuickQAC{app:rcuimpl:Read-Copy Update Implementations}
\QuickQ{Why is sleeping prohibited within Classic RCU read-side
critical sections?}

Because sleeping implies a context switch, which in Classic RCU is
a quiescent state, and RCU's grace-period detection requires that
quiescent states never appear in RCU read-side critical sections.

\QuickQ{Why not permit sleeping in Classic RCU read-side critical sections
by eliminating context switch as a quiescent state, leaving user-mode
execution and idle loop as the remaining quiescent states?}

This would mean that a system undergoing heavy kernel-mode execution load
(e.g., due to kernel threads) might never complete a grace period, which
would cause it to exhaust memory sooner or later.

\QuickQ{Why is it OK to assume that updates separated by
	{\tt synchronize\_sched()} will be performed in order?}

	Because this property is required for the {\tt synchronize\_sched()}
	aspect of RCU to work at all.
	For example, consider a code sequence that removes an object
	from a list, invokes {\tt synchronize\_sched()}, then frees
	the object.
	If this property did not hold, then that object might appear
	to be freed before it was
	removed from the list, which is precisely the situation that
	{\tt synchronize\_sched()} is supposed to prevent!

\QuickQ{Why must line~17 in {\tt synchronize\_srcu()}
	(Figure~\ref{fig:app:rcuimpl:Update-Side Implementation})
	precede the release of the mutex on line~18?
	What would have to change to permit these two lines to be
	interchanged?
	Would such a change be worthwhile?
	Why or why not?}

	Suppose that the order was reversed, and that CPU~0
	has just reached line~13 of
	{\tt synchronize\_srcu()}, while both CPU~1 and CPU~2 start executing
	another {\tt synchronize\_srcu()} each, and CPU~3 starts executing a
	{\tt srcu\_read\_lock()}.
	Suppose that CPU~1 reaches line~6 of {\tt synchronize\_srcu()}
	just before CPU~0 increments the counter on line~13.
	Most importantly, suppose that
	CPU~3 executes {\tt srcu\_read\_lock()}
	out of order with the following SRCU read-side critical section,
	so that it acquires a reference to some SRCU-protected data
	structure \emph{before} CPU~0 increments {\tt sp->completed}, but
	executes the {\tt srcu\_read\_lock()} \emph{after} CPU~0 does
	this increment.
	
	Then CPU~0 will \emph{not} wait for CPU~3 to complete its
	SRCU read-side critical section before exiting the ``while''
	loop on lines~15-16 and releasing the mutex (remember, the
	CPU could be reordering the code).
	
	Now suppose that CPU~2 acquires the mutex next,
	and again increments {\tt sp->completed}.
	This CPU will then have to wait for CPU~3 to exit its SRCU
	read-side critical section before exiting the loop on
	lines~15-16 and releasing the mutex.
	But suppose that CPU~3 again executes out of order,
	completing the {\tt srcu\_read\_unlock()} prior to
	executing a final reference to the pointer it obtained
	when entering the SRCU read-side critical section.

	CPU~1 will then acquire the mutex, but see that the
	{\tt sp->completed} counter has incremented twice, and
	therefore take the early exit.
	The caller might well free up the element that CPU~3 is
	still referencing (due to CPU~3's out-of-order execution).

	To prevent this perhaps improbable, but entirely possible,
	scenario, the final {\tt synchronize\_sched()} must precede
	the mutex release in {\tt synchronize\_srcu()}.

	Another approach would be to change to comparison on
	line~7 of {\tt synchronize\_srcu()} to check for at
	least three increments of the counter.
	However, such a change would increase the latency of a
	``bulk update'' scenario, where a hash table is being updated
	or unloaded using multiple threads.
	In the current code, the latency of the resulting concurrent
	{\tt synchronize\_srcu()} calls would take at most two SRCU
	grace periods, while with this change, three would be required.

	More experience will be required to determine which approach
	is really better.
	For one thing, there must first be some use of SRCU with
	multiple concurrent updaters.

\QuickQ{Wait a minute!  With all those new locks, how do you avoid deadlock?}

Deadlock is avoided by never holding more than one of the
\url{rcu_node} structures' locks at a given time.
This algorithm uses two more locks, one to prevent CPU hotplug operations
from running concurrently with grace-period advancement
(\url{onofflock}) and another
to permit only one CPU at a time from forcing a quiescent state
to end quickly (\url{fqslock}).
These are subject to a locking hierarchy, so that
\url{fqslock} must be acquired before
\url{onofflock}, which in turn must be acquired before
any of the \url{rcu_node} structures' locks.

Also, as a practical matter, refusing to ever hold more than
one of the \url{rcu_node} locks means that it is unnecessary
to track which ones are held.
Such tracking would be painful as well as unnecessary.

\QuickQ{Why stop at a 64-times reduction?
Why not go for a few orders of magnitude instead?}

RCU works with no problems on
systems with a few hundred CPUs, so allowing 64 CPUs to contend on
a single lock leaves plenty of headroom.
Keep in mind that these locks are acquired quite rarely, as each
CPU will check in about one time per grace period, and grace periods
extend for milliseconds.

\QuickQ{But I don't care about McKenney's lame excuses in the answer to
Quick Quiz 2!!!
I want to get the number of CPUs contending on a single lock down
to something reasonable, like sixteen or so!!!}

OK, have it your way, then!!!
Set \url{CONFIG_RCU_FANOUT=16} and (for \url{NR_CPUS=4096})
you will get a
three-level hierarchy with with 256 \url{rcu_node} structures
at the lowest level, 16 \url{rcu_node} structures as intermediate
nodes, and a single root-level \url{rcu_node}.
The penalty you will pay is that more \url{rcu_node} structures
will need to be scanned when checking to see which CPUs need help
completing their quiescent states (256 instead of only 64).

\QuickQ{OK, so what is the story with the colors?}

Data structures analogous to \url{rcu_state} (including
\url{rcu_ctrlblk}) are yellow,
those containing the bitmaps used to determine when CPUs have checked
in are pink,
and the per-CPU \url{rcu_data} structures are blue.
The data structures used to conserve energy
(such as \url{rcu_dynticks}) will be colored green.

\QuickQ{Given such an egregious bug, why does Linux run at all?}

Because the Linux kernel contains device drivers that are (relatively)
well behaved.
Few if any of them spin in RCU read-side critical sections for the
many milliseconds that would be required to provoke this bug.
The bug nevertheless does need to be fixed, and this variant of
RCU does fix it.

\QuickQ{But doesn't this state diagram indicate that dyntick-idle CPUs will
get hit with reschedule IPIs?  Won't that wake them up?}

No.
Keep in mind that RCU is handling groups of CPUs.
One particular group might contain both dyntick-idle CPUs and
CPUs in normal mode that have somehow managed to avoid passing through
a quiescent state.
Only the latter group will be sent a reschedule IPI; the dyntick-idle
CPUs will merely be marked as being in an extended quiescent state.

\QuickQ{But what happens if a CPU tries to report going through a quiescent
state (by clearing its bit) before the bit-setting CPU has finished?}

There are three cases to consider here:

\begin{enumerate}
\item	A CPU corresponding to a non-yet-initialized leaf \url{rcu_node}
	structure tries to report a quiescent state.
	This CPU will see its bit already cleared, so will give up on
	reporting its quiescent state.
	Some later quiescent state will serve for the new grace period.
\item	A CPU corresponding to a leaf \url{rcu_node} structure that
	is currently being initialized tries to report a quiescent state.
	This CPU will see that the \url{rcu_node} structure's
	\url{->lock} is held, so will spin until it is
	released.
	But once the lock is released, the \url{rcu_node}
	structure will have been initialized, reducing to the
	following case.
\item	A CPU corresponding to a leaf \url{rcu_node} that has
	already been initialized tries to report a quiescent state.
	This CPU will find its bit set, and will therefore clear it.
	If it is the last CPU for that leaf node, it will
	move up to the next level of the hierarchy.
	However, this CPU cannot possibly be the last CPU in the system to
	report a quiescent state, given that the CPU doing the initialization
	cannot yet have checked in.
\end{enumerate}

So, in all three cases, the potential race is resolved correctly.

\QuickQ{And what happens if \emph{all} CPUs try to report going
through a quiescent
state before the bit-setting CPU has finished, thus ending the new
grace period before it starts?}

The bit-setting CPU cannot pass through a
quiescent state during initialization, as it has irqs disabled.
Its bits therefore remain non-zero, preventing the grace period from
ending until the data structure has been fully initialized.

\QuickQ{And what happens if one CPU comes out of dyntick-idle mode and then
passed through a quiescent state just as another CPU notices that the
first CPU was in dyntick-idle mode?
Couldn't they both attempt to report a quiescent state at the same
time, resulting in confusion?}

They will both attempt to acquire the lock on the same leaf
\url{rcu_node} structure.
The first one to acquire the lock will report the quiescent state
and clear the appropriate bit, and the second one to acquire the
lock will see that this bit has already been cleared.

\QuickQ{But what if \emph{all} the CPUs end up in dyntick-idle mode?
Wouldn't that prevent the current RCU grace period from ever ending?}

Indeed it will!
However, CPUs that have RCU callbacks are not permitted to enter
dyntick-idle mode, so the only way that \emph{all} the CPUs could
possibly end up in dyntick-idle mode would be if there were
absolutely no RCU callbacks in the system.
And if there are no RCU callbacks in the system, then there is no
need for the RCU grace period to end.
In fact, there is no need for the RCU grace period to even \emph{start}.

RCU will restart if some irq handler does a \url{call_rcu()},
which will cause an RCU callback to appear on the corresponding CPU,
which will force that CPU out of dyntick-idle mode, which will in turn
permit the current RCU grace period to come to an end.

\QuickQ{Given that \url{force_quiescent_state()} is a three-phase state
machine, don't we have triple the scheduling latency due to scanning
all the CPUs?}

Ah, but the three phases will not execute back-to-back on the same CPU,
and, furthermore, the first (initialization) phase doesn't do any scanning.
Therefore, the scheduling-latency hit of the three-phase algorithm is no
different than that of a single-phase algorithm.
If the scheduling latency becomes a problem, one approach would be to
recode the state machine to scan the CPUs incrementally, most likely
by keeping state on a per-leaf-\url{rcu_node} basis.
But first show me a problem in the real world, \emph{then}
I will consider fixing it!

\QuickQ{But the other reason to hold \url{->onofflock} is to prevent
multiple concurrent online/offline operations, right?}

Actually, no!
The CPU-hotplug code's synchronization design prevents multiple
concurrent CPU online/offline operations, so only one CPU online/offline
operation can be executing at any given time.
Therefore, the only purpose of \url{->onofflock} is to prevent a CPU
online or offline operation from running concurrently with grace-period
initialization.

\QuickQ{Given all these acquisitions of the global \url{->onofflock},
won't there
be horrible lock contention when running with thousands of CPUs?}

Actually, there can be only three acquisitions of this lock per grace
period, and each grace period lasts many milliseconds.
One of the acquisitions is by the CPU initializing for the current
grace period, and the other two onlining and offlining some CPU.
These latter two cannot run concurrently due to the CPU-hotplug
locking, so at most two CPUs can be contending for this lock at any
given time.

Lock contention on \url{->onofflock} should therefore
be no problem, even on systems with thousands of CPUs.

\QuickQ{Why not simplify the code by merging the detection of dyntick-idle
CPUs with that of offline CPUs?}

It might well be that such merging may eventually be the right thing to
do.
In the meantime, however, there are some challenges:

\begin{enumerate}
\item	CPUs are not allowed to go into dyntick-idle mode while they
	have RCU callbacks pending, but CPUs \emph{are} allowed to go
	offline with callbacks pending.
	This means that CPUs going offline need to have their callbacks
	migrated to some other CPU, thus, we cannot allow CPUs to simply
	go quietly offline.
\item	Present-day Linux systems run with \url{NR_CPUS}
	much larger than the actual number of CPUs.
	A unified approach could thus end up uselessly waiting on
	CPUs that are not just offline, but which never existed in
	the first place.
\item	RCU is already operational when CPUs get onlined one
	at a time during boot, and therefore must handle the online
	process.
	This onlining must exclude grace-period initialization, so
	the \url{->onofflock} must still be used.
\item	CPUs often switch into and out of dyntick-idle mode
	extremely frequently, so it is not reasonable to use the
	heavyweight online/offline code path for entering and exiting
	dyntick-idle mode.
\end{enumerate}

\QuickQ{Why not simply disable bottom halves (softirq) when acquiring
	the \url{rcu_data} structure's \url{lock}?
	Wouldn't this be faster?}

	Because this lock can be acquired from functions
	called by \url{call_rcu()}, which in turn can be
	invoked from irq handlers.
	Therefore, irqs \emph{must} be disabled when
	holding this lock.

\QuickQ{How about the \url{qsmask} and \url{qsmaskinit}
	fields for the leaf \url{rcu_node} structures?
	Doesn't there have to be some way to work out
	which of the bits in these fields corresponds
	to each CPU covered by the \url{rcu_node} structure
	in question?}

	Indeed there does!
	The \url{grpmask} field in each CPU's \url{rcu_data}
	structure does this job.

\QuickQ{But why bother setting \url{qs_pending} to one when a CPU
	is coming online, given that being offline is an extended
	quiescent state that should cover any ongoing grace period?}

	Because this helps to resolve a race between a CPU coming online
	just as a new grace period is starting.

\QuickQ{Why record the last completed grace period number in
	\url{passed_quiesc_completed}?
	Doesn't that cause this RCU implementation to be vulnerable
	to quiescent states seen while no grace period was in progress
	being incorrectly applied to the next grace period that starts?}

	We record the last completed grace period number in order
	to avoid races where a quiescent state noted near the end of
	one grace period is incorrectly applied to the next grace
	period, especially for dyntick and CPU-offline grace periods.
	Therefore, \url{force_quiescent_state()} and friends all
	check the last completed grace period number to avoid such races.

	Now these dyntick and CPU-offline grace periods are only checked
	for when a grace period is actually active.
	The only quiescent states that can be recorded when no grace
	period is in progress are self-detected quiescent states,
	which are recorded in the \url{passed_quiesc_completed},
	\url{passed_quiesc}, and \url{qs_pending}.
	These variables are initialized every time the corresponding
	CPU notices that a new grace period has started, preventing
	any obsolete quiescent states from being applied to the
	new grace period.

	All that said, optimizing grace-period latency may require that
	\url{gpnum} be tracked in addition to \url{completed}.

\QuickQ{What is the point of running a system with \url{NR_CPUS}
	way bigger than the actual number of CPUs?}

	Because this allows producing a single binary of the Linux kernel
	that runs on a wide variety of systems, greatly easing administration
	and validation.

\QuickQ{Why not simply have multiple lists rather than this funny
	multi-tailed list?}

	Because this multi-tailed approach, due to Lai Jiangshan,
	simplifies callback processing.

\QuickQ{So some poor CPU has to note quiescent states on behalf of
	each and every offline CPU?
	Yecch!
	Won't that result in excessive overheads in the not-uncommon
	case of a system with a small number of CPUs but a large value
	for \url{NR_CPUS}?}

	Actually, no it will not!

	Offline CPUs are excluded from both the \url{qsmask} and
	\url{qsmaskinit} bit masks, so RCU normally ignores them.
	However, there are races with online/offline operations that
	can result in an offline CPU having its \url{qsmask} bit set.
	These races must of course be handled correctly, and the way
	they are handled is to permit other CPUs to note that RCU
	is waiting on a quiescent state from an offline CPU.

\QuickQ{So what guards the earlier fields in this structure?}

	Nothing does, as they are constants set at compile time
	or boot time.
	Of course, the fields internal to each \url{rcu_node}
	in the \url{->node} array may change, but they are
	guarded separately.

\QuickQ{Why not simply use \url{__get_cpu_var()} to pick up a
	reference to the
	current CPU's \url{rcu_data} structure on line~13 in
	Figure~\ref{fig:app:rcuimpl:rcutreewt:Code for rcutree call-rcu}?}

	Because we might be called either from \url{call_rcu()}
	(in which case we would need \url{__get_cpu_var(rcu_data)})
	or from \url{call_rcu_bh()} (in which case we would need
	\url{__get_cpu_var(rcu_bh_data)}).
	Using the \url{->rda[]} array of whichever
	\url{rcu_state} structure we were passed works correctly
	regardless of which API \url{__call_rcu()} was invoked from
	(suggested by Lai Jiangshan).

\QuickQ{Given that \url{rcu_pending()} is always called twice
	on lines~29-32 of
	Figure~\ref{fig:app:rcuimpl:rcutreewt:Code for rcutree rcu-check-callbacks},
	shouldn't there be some way to combine the checks of the
	two structures?}

	Sorry, but this was a trick question.
	The C language's short-circuit boolean expression evaluation
	means that \url{__rcu_pending()} is invoked on
	\url{rcu_bh_state} only if the prior invocation on
	\url{rcu_state} returns zero.

	The reason the two calls are in this order is that
	``rcu'' is used more heavily than is ``rcu\_bh'', so
	the first call is more likely to return non-zero than
	is the second.

\QuickQ{Shouldn't line~43 of
	Figure~\ref{fig:app:rcuimpl:rcutreewt:Code for rcutree rcu-check-callbacks}
	also check for \url{in_hardirq()}?}

	No.
	The \url{rcu_read_lock_bh()} primitive disables
	softirq, not hardirq.
	Because \url{call_rcu_bh()} need only wait for pre-existing
	``rcu\_bh'' read-side critical sections to complete,
	we need only check \url{in_softirq()}.

\QuickQ{Why is it important that blocking primitives
	called from within a preemptible-RCU read-side critical section be
	subject to priority inheritance?}

	Because blocked readers stall RCU grace periods,
	which can result in OOM.
	For example, if a reader did a \url{wait_event()} within
	an RCU read-side critical section, and that event never occurred,
	then RCU grace periods would stall indefinitely, guaranteeing that
	the system would OOM sooner or later.
	There must therefore be some way to cause these readers to progress
	through their read-side critical sections in order to avoid such OOMs.
	Priority boosting is one way to force such progress, but only if
	readers are restricted to blocking such that they can be awakened via
	priority boosting.

	Of course, there are other methods besides priority inheritance
	that handle the priority inversion problem, including priority ceiling,
	preemption disabling, and so on.
	However, there are good reasons why priority inheritance is the approach
	used in the Linux kernel, so this is what is used for RCU.

\QuickQ{Could the prohibition against using primitives
	that would block in a non-\url{CONFIG_PREEMPT} kernel be lifted,
	and if so, under what conditions?}

	If testing and benchmarking demonstrated that the
	preemptible RCU worked well enough that classic RCU could be dispensed
	with entirely, and if priority inheritance was implemented for blocking
	synchronization primitives
	such as \url{semaphore}s, then those primitives could be
	used in RCU read-side critical sections.

\QuickQ{How is it possible for lines~38-43 of
	\url{__rcu_advance_callbacks()} to be executed when
	lines~7-37 have not?
	Won't they both be executed just after a counter flip, and
	never at any other time?}

Consider the following sequence of events:
\begin{enumerate}
\item	CPU 0 executes lines~5-12 of
	\url{rcu_try_flip_idle()}.
\item	CPU 1 executes \url{__rcu_advance_callbacks()}.
	Because \url{rcu_ctrlblk.completed} has been
	incremented, lines~7-37 execute.
	However, none of the \url{rcu_flip_flag} variables
	have been set, so lines~38-43 do \emph{not} execute.
\item	CPU 0 executes lines~13-15 of
	\url{rcu_try_flip_idle()}.
\item	Later, CPU 1 again executes \url{__rcu_advance_callbacks()}.
	The counter has not been incremented since the earlier
	execution, but the \url{rcu_flip_flag} variables have
	all been set, so only lines~38-43 are executed.
\end{enumerate}

\QuickQ{What problems could arise if the lines containing
	\url{ACCESS_ONCE()} in \url{rcu_read_unlock()}
	were reordered by the compiler?}

\begin{enumerate}
\item	If the \url{ACCESS_ONCE()} were omitted from the
	fetch of \url{rcu_flipctr_idx} (line~14), then the compiler
	would be within its rights to eliminate \url{idx}.
	It would also be free to compile the \url{rcu_flipctr}
	decrement as a fetch-increment-store sequence, separately fetching
	\url{rcu_flipctr_idx} for both the fetch and the store.
	If an NMI were to occur between the fetch and the store, and
	if the NMI handler contained an \url{rcu_read_lock()},
	then the value of \url{rcu_flipctr_idx} would change
	in the meantime, resulting in corruption of the
	\url{rcu_flipctr} values, destroying the ability
	to correctly identify grace periods.
\item	Another failure that could result from omitting the
	\url{ACCESS_ONCE()} from line~14 is due to
	the compiler reordering this statement to follow the
	decrement of \url{rcu_read_lock_nesting}
	(line~16).
	In this case, if an NMI were to occur between these two
	statements, then any \url{rcu_read_lock()} in the
	NMI handler could corrupt \url{rcu_flipctr_idx},
	causing the wrong \url{rcu_flipctr} to be
	decremented.
	As with the analogous situation in \url{rcu_read_lock()},
	this could result in premature grace-period termination,
	an indefinite grace period, or even both.
\item	If \url{ACCESS_ONCE()} macros were omitted such that
	the update of \url{rcu_read_lock_nesting} could be
	interchanged by the compiler with the decrement of
	\url{rcu_flipctr}, and if an NMI occurred in between,
	any \url{rcu_read_lock()} in the NMI handler would
	incorrectly conclude that it was protected by an enclosing
	\url{rcu_read_lock()}, and fail to increment the
	\url{rcu_flipctr} variables.
\end{enumerate}

It is not clear that the \url{ACCESS_ONCE()} on the
fetch of \url{rcu_read_lock_nesting} (line~7) is required.

\QuickQ{What problems could arise if the lines containing
	\url{ACCESS_ONCE()} in \url{rcu_read_unlock()}
	were reordered by the CPU?}

	Absolutely none!!!  The code in \url{rcu_read_unlock()}
	interacts with the scheduling-clock interrupt handler
	running on the same CPU, and is thus insensitive to reorderings
	because CPUs always see their own accesses as if they occurred
	in program order.
	Other CPUs do access the \url{rcu_flipctr}, but because these
	other CPUs don't access any of the other variables, ordering is
	irrelevant.

\QuickQ{What problems could arise in
	\url{rcu_read_unlock()} if irqs were not disabled?}

\begin{enumerate}
\item	Disabling irqs has the side effect of disabling preemption.
	Suppose that this code were to be preempted in the midst
	of line~17 between selecting the current CPU's copy
	of the \url{rcu_flipctr} array and the decrement of
	the element indicated by \url{rcu_flipctr_idx}.
	Execution might well resume on some other CPU.
	If this resumption happened concurrently with an
	\url{rcu_read_lock()} or \url{rcu_read_unlock()}
	running on the original CPU,
	an increment or decrement might be lost, resulting in either
	premature termination of a grace period, indefinite extension
	of a grace period, or even both.
\item	Failing to disable preemption can also defeat RCU priority
	boosting, which relies on \url{rcu_read_lock_nesting}
	to determine which tasks to boost.
	If preemption occurred between the update of
	\url{rcu_read_lock_nesting} (line~16) and of
	\url{rcu_flipctr} (line~17), then a grace
	period might be stalled until this task resumed.
	But because the RCU priority booster has no way of knowing
	that this particular task is stalling grace periods, needed
	boosting will never occur.
	Therefore, if there are CPU-bound realtime tasks running,
	the preempted task might never resume, stalling grace periods
	indefinitely, and eventually resulting in OOM.
\end{enumerate}

Of course, both of these situations could be handled by disabling
preemption rather than disabling irqs.
(The CPUs I have access to do not show much difference between these
two alternatives, but others might.)

\QuickQ{Suppose that the irq disabling in
	\url{rcu_read_lock()} was replaced by preemption disabling.
	What effect would that have on \url{GP_STAGES}?}

No finite value of \url{GP_STAGES} suffices.
The following scenario, courtesy of Oleg Nesterov, demonstrates this:

Suppose that low-priority Task~A has executed
\url{rcu_read_lock()} on CPU 0,
and thus has incremented \url{per_cpu(rcu_flipctr, 0)[0]},
which thus has a value of one.
Suppose further that Task~A is now preempted indefinitely.

Given this situation, consider the following sequence of events:
\begin{enumerate}
\item	Task~B starts executing \url{rcu_read_lock()}, also on
	CPU 0, picking up the low-order bit of
	\url{rcu_ctrlblk.completed}, which is still equal to zero.
\item	Task~B is interrupted by a sufficient number of scheduling-clock
	interrupts to allow the current grace-period stage to complete,
	and also be sufficient long-running interrupts to allow the
	RCU grace-period state machine to advance the
	\url{rcu_ctrlblk.complete} counter so that its bottom bit
	is now equal to one and all CPUs have acknowledged this increment
	operation.
\item	CPU 1 starts summing the index==0 counters, starting with
	\url{per_cpu(rcu_flipctr, 0)[0]}, which is equal to one
	due to Task~A's increment.
	CPU 1's local variable \url{sum} is therefore equal to one.
\item	Task~B returns from interrupt, resuming its execution of
	\url{rcu_read_lock()}, incrementing
	\url{per_cpu(rcu_flipctr, 0)[0]}, which now has a value
	of two.
\item	Task~B is migrated to CPU 2.
\item	Task~B completes its RCU read-side critical section, and executes
	\url{rcu_read_unlock()}, which decrements
	\url{per_cpu(rcu_flipctr, 2)[0]}, which is now -1.
\item	CPU 1 now adds \url{per_cpu(rcu_flipctr, 1)[0]} and 
	\url{per_cpu(rcu_flipctr, 2)[0]} to its
	local variable \url{sum}, obtaining the value zero.
\item	CPU 1 then incorrectly concludes that all prior RCU read-side
	critical sections have completed, and advances to the next
	RCU grace-period stage.
	This means that some other task might well free up data structures
	that Task~A is still using!
\end{enumerate}

This sequence of events could repeat indefinitely, so that no finite
value of \url{GP_STAGES} could prevent disrupting Task~A.
This sequence of events demonstrates the importance of the promise
made by CPUs that acknowledge an increment of
\url{rcu_ctrlblk.completed}, as the problem illustrated by the
above sequence of events is caused by Task~B's repeated failure
to honor this promise.

Therefore, more-pervasive changes to the grace-period state will be
required in order for \url{rcu_read_lock()} to be able to safely
dispense with irq disabling.

\QuickQ{Why can't the \url{rcu_dereference()}
	precede the memory barrier?}

	Because the memory barrier is being executed in
	an interrupt handler, and interrupts are exact in the sense that
	a single value of the PC is saved upon interrupt, so that the
	interrupt occurs at a definite place in the code.
	Therefore, if the
	\url{rcu_dereference()} were to precede the memory barrier,
	the interrupt would have had to have occurred after the
	\url{rcu_dereference()}, and therefore
	the interrupt would also have had to have occurred after the
	\url{rcu_read_lock()} that begins the RCU read-side critical
	section.
	This would have forced the \url{rcu_read_lock()} to use
	the earlier value of the grace-period counter, which would in turn
	have meant that the corresponding \url{rcu_read_unlock()}
	would have had to precede the first "Old counters zero [0]" rather
	than the second one.
	This in turn would have meant that the read-side critical section
	would have been much shorter --- which would have been
	counter-productive,
	given that the point of this exercise was to identify the longest
	possible RCU read-side critical section.

\QuickQ{What is a more precise way to say "CPU~0
	might see CPU~1's increment as early as CPU~1's last previous
	memory barrier"?}

	First, it is important to note that the problem with
	the less-precise statement is that it gives the impression that there
	might be a single global timeline, which there is not, at least not for
	popular microprocessors.
	Second, it is important to note that memory barriers are all about
	perceived ordering, not about time.
	Finally, a more precise way of stating above statement would be as
	follows: "If CPU~0 loads the value resulting from CPU~1's
	increment, then any subsequent load by CPU~0 will see the
	values from any relevant stores by CPU~1 if these stores
	preceded CPU~1's last prior memory barrier."

	Even this more-precise version leaves some wiggle room.
	The word "subsequent" must be understood to mean "ordered after",
	either by an explicit memory barrier or by the CPU's underlying
	memory ordering.
	In addition, the memory barriers must be strong enough to order
	the relevant operations.
	For example, CPU~1's last prior memory barrier must order stores
	(for example, \url{smp_wmb()} or \url{smp_mb()}).
	Similarly, if CPU~0 needs an explicit memory barrier to
	ensure that its later load follows the one that saw the increment,
	then this memory barrier needs to be an \url{smp_rmb()}
	or \url{smp_mb()}.

	In general, much care is required when proving parallel algorithms.

\QuickQAC{app:formal:Formal Verification}
\QuickQ{Why is there an unreached statement in
locker?  After all, isn't this a \emph{full} state-space
search???}

The locker process is an infinite loop, so control
never reaches the end of this process.
However, since there are no monotonically increasing variables,
Promela is able to model this infinite loop with a small
number of states.

\QuickQ{What are some Promela code-style issues with this example?}

There are several:
\begin{enumerate}
\item	The declaration of {\tt sum} should be moved to within
	the init block, since it is not used anywhere else.
\item	The assertion code should be moved outside of the
	initialization loop.  The initialization loop can
	then be placed in an atomic block, greatly reducing
	the state space (by how much?).
\item	The atomic block covering the assertion code should
	be extended to include the initialization of {\tt sum}
	and {\tt j}, and also to cover the assertion.
	This also reduces the state space (again, by how
	much?).
\end{enumerate}

\QuickQ{Is there a more straightforward way to code the do-od statement?}

Yes.  Replace it with {\tt if-fi} and remove the two {\tt break} statements.

\QuickQ{Why are there atomic blocks at lines 12-21
and lines 44-56, when the operations within those atomic
blocks have no atomic implementation on any current
production microprocessor?}

Because those operations are for the benefit of the
assertion only.  They are not part of the algorithm itself.
There is therefore no harm in marking them atomic, and
so marking them greatly reduces the state space that must
be searched by the Promela model.

\QuickQ{Is the re-summing of the counters on lines 24-27
\emph{really} necessary???}

Yes.  To see this, delete these lines and run the model.

Alternatively, consider the following sequence of steps:

\begin{enumerate}
\item	One process is within its RCU read-side critical
	section, so that the value of {\tt ctr[0]} is zero and
	the value of {\tt ctr[1]} is two.
\item	An updater starts executing, and sees that the sum of
	the counters is two so that the fastpath cannot be
	executed.  It therefore acquires the lock.
\item	A second updater starts executing, and fetches the value
	of {\tt ctr[0]}, which is zero.
\item	The first updater adds one to {\tt ctr[0]}, flips
	the index (which now becomes zero), then subtracts
	one from {\tt ctr[1]} (which now becomes one).
\item	The second updater fetches the value of {\tt ctr[1]},
	which is now one.
\item	The second updater now incorrectly concludes that it
	is safe to proceed on the fastpath, despite the fact
	that the original reader has not yet completed.
\end{enumerate}

\QuickQ{Yeah, that's great!!!
	Now, just what am I supposed to do if I don't happen to have a
	machine with 40GB of main memory???}

	Relax, there are a number of lawful answers to
	this question:
	\begin{enumerate}
	\item	Further optimize the model, reducing its memory consumption.
	\item	Work out a pencil-and-paper proof, perhaps starting with the
		comments in the code in the Linux kernel.
	\item	Devise careful torture tests, which, though they cannot prove
		the code correct, can find hidden bugs.
	\item	There is some movement towards tools that do model
		checking on clusters of smaller machines.
		However, please note that we have not actually used such
		tools myself, courtesy of some large machines that Paul has
		occasional access to.
	\end{enumerate}

\QuickQ{Why not simply increment \url{rcu_update_flag}, and then only
	increment \url{dynticks_progress_counter} if the old value
	of \url{rcu_update_flag} was zero???}

	This fails in presence of NMIs.
	To see this, suppose an NMI was received just after
	\url{rcu_irq_enter()} incremented \url{rcu_update_flag},
	but before it incremented \url{dynticks_progress_counter}.
	The instance of \url{rcu_irq_enter()} invoked by the NMI
	would see that the original value of \url{rcu_update_flag}
	was non-zero, and would therefore refrain from incrementing
	\url{dynticks_progress_counter}.
	This would leave the RCU grace-period machinery no clue that the
	NMI handler was executing on this CPU, so that any RCU read-side
	critical sections in the NMI handler would lose their RCU protection.

	The possibility of NMI handlers, which, by definition cannot
	be masked, does complicate this code.

\QuickQ{But if line~7 finds that we are the outermost interrupt,
	wouldn't we \emph{always} need to increment
	\url{dynticks_progress_counter}?}

	Not if we interrupted a running task!
	In that case, \url{dynticks_progress_counter} would
	have already been incremented by \url{rcu_exit_nohz()},
	and there would be no need to increment it again.

\QuickQ{Can you spot any bugs in any of the code in this section?}

	Read the next section to see if you were correct.

\QuickQ{Why isn't the memory barrier in \url{rcu_exit_nohz()}
	and \url{rcu_enter_nohz()} modeled in Promela?}

	Promela assumes sequential consistency, so
	it is not necessary to model memory barriers.
	In fact, one must instead explicitly model lack of memory barriers,
	for example, as shown in
	Figure~\ref{fig:analysis:QRCU Unordered Summation} on
	page~\pageref{fig:analysis:QRCU Unordered Summation}.

\QuickQ{Isn't it a bit strange to model \url{rcu_exit_nohz()}
	followed by \url{rcu_enter_nohz()}?
	Wouldn't it be more natural to instead model entry before exit?}

	It probably would be more natural, but we will need
	this particular order for the liveness checks that we will add later.

\QuickQ{Wait a minute!
	In the Linux kernel, both \url{dynticks_progress_counter} and
	\url{rcu_dyntick_snapshot} are per-CPU variables.
	So why are they instead being modeled as single global variables?}

	Because the grace-period code processes each
	CPU's \url{dynticks_progress_counter} and
	\url{rcu_dyntick_snapshot} variables separately,
	we can collapse the state onto a single CPU.
	If the grace-period code were instead to do something special
	given specific values on specific CPUs, then we would indeed need
	to model multiple CPUs.
	But fortunately, we can safely confine ourselves to two CPUs, the
	one running the grace-period processing and the one entering and
	leaving dynticks-idle mode.

\QuickQ{Given there are a pair of back-to-back changes to
	\url{grace_period_state} on lines~25 and 26,
	how can we be sure that line~25's changes won't be lost?}

	Recall that Promela and spin trace out
	every possible sequence of state changes.
	Therefore, timing is irrelevant: Promela/spin will be quite
	happy to jam the entire rest of the model between those two
	statements unless some state variable specifically prohibits
	doing so.

\QuickQ{But what would you do if you needed the statements in a single
	\url{EXECUTE_MAINLINE()} group to execute non-atomically?}

	The easiest thing to do would be to put
	each such statement in its own \url{EXECUTE_MAINLINE()}
	statement.

\QuickQ{But what if the \url{dynticks_nohz()} process had
	``if'' or ``do'' statements with conditions,
	where the statement bodies of these constructs
	needed to execute non-atomically?}

	One approach, as we will see in a later section,
	is to use explicit labels and ``goto'' statements.
	For example, the construct:

	\vspace{5pt}
	\begin{minipage}[t]{\columnwidth}
	\scriptsize
	\begin{verbatim}
		if
		:: i == 0 -> a = -1;
		:: else -> a = -2;
		fi;
	\end{verbatim}
	\end{minipage}
	\vspace{5pt}

	could be modeled as something like:

	\vspace{5pt}
	\begin{minipage}[t]{\columnwidth}
	\scriptsize
	\begin{verbatim}
		EXECUTE_MAINLINE(stmt1,
				 if
				 :: i == 0 -> goto stmt1_then;
				 :: else -> goto stmt1_else;
				 fi)
		stmt1_then: skip;
		EXECUTE_MAINLINE(stmt1_then1, a = -1; goto stmt1_end)
		stmt1_else: skip;
		EXECUTE_MAINLINE(stmt1_then1, a = -2)
		stmt1_end: skip;
	\end{verbatim}
	\end{minipage}
	\vspace{5pt}

	However, it is not clear that the macro is helping much in the case
	of the ``if'' statement, so these sorts of situations will
	be open-coded in the following sections.

\QuickQ{Why are lines~45 and 46 (the \url{in_dyntick_irq = 0;}
	and the \url{i++;}) executed atomically?}

	These lines of code pertain to controlling the
	model, not to the code being modeled, so there is no reason to
	model them non-atomically.
	The motivation for modeling them atomically is to reduce the size
	of the state space.

\QuickQ{What property of interrupts is this \url{dynticks_irq()}
	process unable to model?}

	One such property is nested interrupts,
	which are handled in the following section.

\QuickQ{Does Paul always write his code in this painfully incremental
	manner???}

	Not always, but more and more frequently.
	In this case, Paul started with the smallest slice of code that included
	an interrupt handler, because he was not sure how best to model interrupts
	in Promela.
	Once he got that working, he added other features.
	(But if he was doing it again, he would start with a ``toy'' handler.
	For example, he might have the handler increment a variable twice and
	have the mainline code verify that the value was always even.)

	Why the incremental approach?
	Consider the following, attributed to Brian W. Kernighan:

	\begin{quote}
		Debugging is twice as hard as writing the code in the first
		place. Therefore, if you write the code as cleverly as possible,
		you are, by definition, not smart enough to debug it.
	\end{quote}

	This means that any attempt to optimize the production of code should
	place at least 66\% of its emphasis on optimizing the debugging process,
	even at the expense of increasing the time and effort spent coding.
	Incremental coding and testing is one way to optimize the debugging
	process, at the expense of some increase in coding effort.
	Paul uses this approach because he rarely has the luxury of
	devoting full days (let alone weeks) to coding and debugging.

\QuickQ{But what happens if an NMI handler starts running before
	an irq handler completes, and if that NMI handler continues
	running until a second irq handler starts?}

	This cannot happen within the confines of a single CPU.
	The first irq handler cannot complete until the NMI handler
	returns.
	Therefore, if each of the \url{dynticks} and \url{dynticks_nmi}
	variables have taken on an even value during a given time
	interval, the corresponding CPU really was in a quiescent
	state at some time during that interval.

\QuickQ{This is still pretty complicated.
	Why not just have a \url{cpumask_t} that has a bit set for
	each CPU that is in dyntick-idle mode, clearing the bit
	when entering an irq or NMI handler, and setting it upon
	exit?}

	Although this approach would be functionally correct, it
	would result in excessive irq entry/exit overhead on
	large machines.
	In contrast, the approach laid out in this section allows
	each CPU to touch only per-CPU data on irq and NMI entry/exit,
	resulting in much lower irq entry/exit overhead, especially
	on large machines.

